{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets Extend Our Sequential class to support derviative for RELU, TanH , LeakyRELU. Here each activation function they have thier own way of derviatives. For Example for sigmoid it is z*(1-z) where z =Sigmoid(w h(x)) . How we derive this equation for derivative is out of scope for this session. After extending this, we try to build our sequential class to recognize handwritten characters from image. Lets copy our Sequential class and its dependency from last session and extending the same .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#This cell block has the List of Activation Functions with derivatives\n",
    "class sigmoid():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self,h):\n",
    "        return 1/(1+np.exp(-h)) \n",
    "    \n",
    "    def derivative(self,h):\n",
    "        return h * (1-h)   \n",
    "    \n",
    "    def diff(self, h ,y):\n",
    "        return y - h\n",
    "\n",
    "class relu():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self,h):\n",
    "        return h * (h >0)\n",
    "    \n",
    "    def derivative(self,h):\n",
    "        return 1. * (h >0)\n",
    "\n",
    "\n",
    "class tanh():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self,h):\n",
    "        return np.tanh(h)\n",
    "    \n",
    "    def derivative(self,h):\n",
    "        return 1. - np.power(h,2)\n",
    "\n",
    "class leakyrelu():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self,h, alpha=0.1):\n",
    "        return np.maximum(h ,alpha)\n",
    "    \n",
    "    def derivative(self,h):\n",
    "        d =  1. (h>0)\n",
    "        d[d <= 0] = alpha\n",
    "        return d       \n",
    "#softmax function as an activation function that maps the accumulated evidences to a probability distribution over the classes\n",
    "class softmax():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def __call__(self,h):\n",
    "        expo = np.exp(h)\n",
    "        result = expo/np.sum(expo,axis=1, keepdims=True)  \n",
    "        return result\n",
    "    \n",
    "    def derivative(self,h):\n",
    "        return None\n",
    "    \n",
    "    def diff(self, h, y):\n",
    "        dscore = h\n",
    "        dscore[range(len(y)), y.ravel()-1] -= 1        \n",
    "        dscore /= len(y)\n",
    "        return dscore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of loss functions listed below, for Logistic Regression binary loss will be the default choice, for softmax predictions, cross entropy will be the right one. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This Cell Block has List of Loss functions\n",
    "def binaryLoss(y, p,lweights = None, reg=1e-3):\n",
    "    return np.mean(-(y * np.log(p) + (1-y)*np.log(1-p)))    \n",
    "#y -actual output p - predicted output reg - regularization strength\n",
    "def crossEntropyForSoftMax(y, p,lweights = None, reg=1e-3): \n",
    "    #select the right propbolity for loss  \n",
    "    correct_prob = -np.log(p[range(len(y)), y.ravel()-1])\n",
    "    dataloss = np.sum(correct_prob)/len(y)   \n",
    "    \n",
    "    #regularization can be defined by 1/2 * Reg * np.sum(w*2)\n",
    "    regloss= 0\n",
    "    \n",
    "    if lweights is not None:\n",
    "        for weight in lweights:\n",
    "            regloss +=  0.5*reg* np.sum(np.square(weight))\n",
    "        \n",
    "    return dataloss+regloss    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential class is a abstracted class to create a network faster. This class can be further optimized by choosing the right datastructure like dictionary to store any computations which helps in doing any computations just once. But for the simplicity , taking a middle ground here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    def __init__(self, layers, epochs, lr,  loss = binaryLoss, reg =1e-3):\n",
    "        self.layers,self.epochs, self.lr,self.loss,self.reg = layers,epochs, lr ,loss,reg\n",
    "        \n",
    "    def __call__(self, X, y,X_valid, y_valid): \n",
    "        #assign weights\n",
    "        self.X,self.y,self.X_valid,self.y_valid = X,y,X_valid, y_valid    \n",
    "        \n",
    "        inputdim = X.shape[1]\n",
    "        np.random.seed(0)     \n",
    "        #initialize layers\n",
    "        for layer in self.layers:\n",
    "            inputdim = layer(inputdim,self.lr, self.reg) \n",
    "            \n",
    "        return self    \n",
    "    \n",
    "    def predict(self, X_input, y_input):\n",
    "        \n",
    "        h = X_input  \n",
    "        layerweigths = []\n",
    "        #compute hidden units\n",
    "        for layer in self.layers:  \n",
    "            h = layer.forward(h) \n",
    "            layerweigths.append(layer.w)\n",
    "        \n",
    "        loss = self.loss(y_input, h, layerweigths, self.reg)         \n",
    "        \n",
    "        return h,loss \n",
    "    \n",
    "    \n",
    "          \n",
    "    def fit(self):  \n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            \n",
    "            if((i%1000) == 0):\n",
    "                valid_h, valid_loss = self.predict(self.X_valid,self.y_valid)     \n",
    "                h, loss = self.predict(self.X,self.y)  \n",
    "                train_accuracy = np.mean(np.argmax(h ,axis=1)+1 == self.y.ravel())\n",
    "                val_accuracy = np.mean(np.argmax(valid_h ,axis=1)+1 == self.y_valid.ravel())\n",
    "               \n",
    "                print(f'Epoch# {i} Training Loss:{loss} Validation Loss: {valid_loss} Training Accuracy:{train_accuracy} Validation Accuracy:{val_accuracy}') \n",
    "            else :    h, loss = self.predict(self.X,self.y)  \n",
    "                \n",
    "            #compute the error  \n",
    "            error = self.layers[-1].activation.diff(h, self.y) \n",
    "            \n",
    "            #back propagate the error - this formula is influenced by andrew ng course  \n",
    "            for i in reversed(range(0,len(self.layers))):  \n",
    "                h = self.X if i == 0 else self.layers[i-1].h\n",
    "                error = self.layers[i].backward(error, h)          \n",
    "               \n",
    "            #update the weights   \n",
    "            for layer in reversed(self.layers):\n",
    "                layer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Layer(ABC):\n",
    "    def __init__(self, outdim, activation = sigmoid): \n",
    "        self.outdim = outdim\n",
    "        self.activation = activation\n",
    "          \n",
    "        \n",
    "    def __call__(self, inputdim, lr= 0.01, reg=1e-3):  \n",
    "        self.inputdim,self.lr,self.reg  = inputdim,lr,reg\n",
    "        self.w = ( 0.01 * np.random.random((self.inputdim, self.outdim)))\n",
    "        self.b = (0.01 * np.random.random((1,self.outdim)))       \n",
    "        return self.outdim\n",
    "        \n",
    "   \n",
    "    def forward(self, x):\n",
    "        self.h = self.activation(x)    \n",
    "        return self.h\n",
    "    \n",
    "    def backward(self, d, h):   \n",
    "        #given an output value from a neuron, we need to calculate itâ€™s slope.\n",
    "      \n",
    "        #Apply the derivative of our activation function to the output layer error\n",
    "        derivative = self.activation.derivative(self.h)\n",
    "        \n",
    "        if(derivative is None):  delta = d \n",
    "        else: delta = d * derivative\n",
    "            \n",
    "        self.dw = np.dot(h.T, delta)\n",
    "        self.db = np.sum(delta, axis =0, keepdims=True)      \n",
    "        \n",
    "        #Use the delta output  to figure out how much our hidden layer contributed to the output error \n",
    "        #by performing a dot product with our weight matrix\n",
    "        error = delta.dot(self.w.T)               \n",
    "        \n",
    "        self.dw += self.reg * self.w    \n",
    "        \n",
    "        return error\n",
    "    \n",
    "    def step(self):      \n",
    "        self.w +=  -self.lr * self.dw\n",
    "        self.b +=  -self.lr * self.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, outdim, activation = sigmoid):        \n",
    "        super().__init__(outdim,activation)        \n",
    "      \n",
    "        \n",
    "    def forward(self,x):        \n",
    "        #linear \n",
    "        h = np.dot(x, self.w)  + self.b\n",
    "        return super().forward(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Prepare the Input for Sequential class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 400)\n",
      "(5000, 1)\n"
     ]
    }
   ],
   "source": [
    "data = loadmat(\"data\\ex3data1.mat\")\n",
    "print(data['X'].shape)\n",
    "print(data['y'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot one image from input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show(img, title:None):\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    if(title is not None): plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAEICAYAAACj9mr/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwsySHBBmluSAMLMkB4SZJf0/ZE4Quqx+he0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23662dea5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#reshape by 20*20\n",
    "image = np.reshape(data['X'][4000],(20,20))\n",
    "image.shape\n",
    "show(image, data['y'][4000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert the First Column as Ones for Bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data['X']\n",
    "y =  data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 400)\n",
      "(1000, 400)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            X,y, test_size=0.20, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([    \n",
    "    Dense(100, activation = relu()),\n",
    "    Dense(10, activation = softmax())    \n",
    "], epochs =10000, lr= 0.1, reg= 1e-3, loss= crossEntropyForSoftMax)(X_train,y_train,X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 0 Training Loss:2.3036310355372773 Validation Loss: 2.3041550030303695 Training Accuracy:0.10475 Validation Accuracy:0.085\n",
      "Epoch# 1000 Training Loss:0.2948425367948705 Validation Loss: 0.3996565177455286 Training Accuracy:0.93075 Validation Accuracy:0.907\n",
      "Epoch# 2000 Training Loss:0.2308697192764801 Validation Loss: 0.3840779523124036 Training Accuracy:0.95175 Validation Accuracy:0.92\n",
      "Epoch# 3000 Training Loss:0.19830037834508418 Validation Loss: 0.38122552253673886 Training Accuracy:0.9685 Validation Accuracy:0.919\n",
      "Epoch# 4000 Training Loss:0.17708025333789545 Validation Loss: 0.3790356183890529 Training Accuracy:0.977 Validation Accuracy:0.923\n",
      "Epoch# 5000 Training Loss:0.16288755469974536 Validation Loss: 0.37694162154958005 Training Accuracy:0.983 Validation Accuracy:0.922\n",
      "Epoch# 6000 Training Loss:0.15380883906196735 Validation Loss: 0.37555439485528996 Training Accuracy:0.9895 Validation Accuracy:0.926\n",
      "Epoch# 7000 Training Loss:0.14805186356499744 Validation Loss: 0.37380542800325733 Training Accuracy:0.994 Validation Accuracy:0.926\n",
      "Epoch# 8000 Training Loss:0.14438294590510953 Validation Loss: 0.372050384544171 Training Accuracy:0.99575 Validation Accuracy:0.93\n",
      "Epoch# 9000 Training Loss:0.14188278372593305 Validation Loss: 0.3697176141653569 Training Accuracy:0.997 Validation Accuracy:0.933\n"
     ]
    }
   ],
   "source": [
    "model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Still the accuracy is improving and that's the strength of neural network. In the next session, lets further extend our neural network to support non linear features like drop out and batch norm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
