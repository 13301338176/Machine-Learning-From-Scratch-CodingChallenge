{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ipdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a Toy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[0,0,1],[0,1,1],[1,0,1],[1,1,1]])\n",
    "y = np.array([[0],[1],[1],[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:0.496410031903\n",
      "Error:0.00858452565325\n",
      "Error:0.00578945986251\n",
      "Error:0.00462917677677\n",
      "Error:0.00395876528027\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# randomly initialize our weights with mean 0\n",
    "syn0 = 2*np.random.random((3,4)) - 1\n",
    "syn1 = 2*np.random.random((4,1)) - 1\n",
    "\n",
    "for j in range(50000):\n",
    "\n",
    "    # Feed forward through layers 0, 1, and 2\n",
    "    l0 = X\n",
    "    l1 = sigmoid(np.dot(l0,syn0))\n",
    "    l2 = sigmoid(np.dot(l1,syn1))\n",
    "\n",
    "    # how much did we miss the target value?\n",
    "    l2_error = y - l2  \n",
    " \n",
    "    if (j% 10000) == 0:\n",
    "        print(\"Error:\" + str(np.mean(np.abs(l2_error))))\n",
    "        \n",
    "    # in what direction is the target value?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "    l2_delta = l2_error*(l2* (1-l2))      \n",
    "    \n",
    "    # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
    "    l1_error = l2_delta.dot(syn1.T)\n",
    "    \n",
    "    \n",
    "    # in what direction is the target l1?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "    l1_delta = l1_error * (l1*(1-l1))\n",
    "   \n",
    "    syn1 += l1.T.dot(l2_delta)\n",
    "    syn0 += l0.T.dot(l1_delta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now our plan to make this piece more abstract and make it a way for easy extension. The objective here to create a sequential class where the network is create with lot of abstraction. For Example                                                model = Sequential([Dense(4),Dense(1,activation = sigmoid) ], epochs =10000, lr= 0.01)(X,y)                                          model.fit()                                                                                                                                                                                    Sequential takes 2 parameters layers list, no of iteration(epochs) , learning rate                                                              Dense layer is the linear layer followed activation, the activation default is Sigmoid can be passed if anything     different. The core idea here to explore the popular activation, loss and extend the neural network to use in a efficient way     \n",
    "\n",
    " \n",
    "  \n",
    "   \n",
    "  \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This cell block has the List of Activation Functions\n",
    "def sigmoid(h):\n",
    "    return 1/(1+np.exp(-h)) \n",
    "\n",
    "def relu(h):\n",
    "    return h * (h >0)\n",
    "\n",
    "def tanh(h):\n",
    "    pass\n",
    "\n",
    "def leakyrelu(h):\n",
    "    pass\n",
    "\n",
    "def elu(h):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This Cell Block has List of Loss functions\n",
    "def binaryLoss(y, p, c= None):\n",
    "    return np.mean(-(y * np.log(p) + (1-y)*np.log(1-p)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    def __init__(self, layers, epochs, lr, loss = binaryLoss):\n",
    "        self.layers,self.epochs, self.lr,self.loss = layers,epochs, lr ,loss\n",
    "        \n",
    "    def __call__(self, X, y): \n",
    "        #assign weights\n",
    "        self.X,self.y = X,y    \n",
    "        \n",
    "        inputdim = X.shape[1]\n",
    "        np.random.seed(0)     \n",
    "        #initialize layers\n",
    "        for layer in self.layers:\n",
    "            inputdim = layer(inputdim,self.lr) \n",
    "            \n",
    "        return self    \n",
    "          \n",
    "    def fit(self):  \n",
    "        for i in range(self.epochs):\n",
    "            h = self.X\n",
    "          \n",
    "            #compute hidden units\n",
    "            for layer in self.layers:                 \n",
    "                h = layer.forward(h)              \n",
    "        \n",
    "            loss = self.loss(self.y, h)  \n",
    "            \n",
    "        \n",
    "            error = self.y - h             \n",
    "            if((i%1000) == 0):\n",
    "                errorrate = np.mean(np.abs(error))                  \n",
    "                print(f'Epoch# {i} Loss:{loss} Error: {errorrate}') \n",
    "            \n",
    "            #back propagate the error - this formula is influenced by andrew ng course  \n",
    "            for layer in reversed(self.layers):                                 \n",
    "                error = layer.backward(error)          \n",
    "               \n",
    "                \n",
    "            for i in reversed(range(0,len(self.layers))): \n",
    "                h = self.X if i == 0 else self.layers[i-1].h\n",
    "                self.layers[i].step(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Layer(ABC):\n",
    "    def __init__(self, outdim, activation = sigmoid): \n",
    "        self.outdim = outdim\n",
    "        self.activation = activation\n",
    "          \n",
    "        \n",
    "    def __call__(self, inputdim, lr):  \n",
    "        self.inputdim,self.lr  = inputdim,lr\n",
    "        self.w = (2*np.random.random((self.inputdim, self.outdim))) - 1\n",
    "        self.lr = lr\n",
    "        return self.outdim\n",
    "        \n",
    "   \n",
    "    def forward(self, x):\n",
    "        self.h = self.activation(x)    \n",
    "        return self.h\n",
    "    \n",
    "    def backward(self, error):   \n",
    "        #given an output value from a neuron, we need to calculate itâ€™s slope.\n",
    "        #We are using the sigmoid transfer function, the derivative of which can be calculated as follows:\n",
    "        #h * (1- h)\n",
    "        \n",
    "        #If you look at below equation - error = ( output - input)   (self.h * (1 - self.h)) - slope of output\n",
    "        #Apply the derivative of our sigmoid activation function to the output layer error\n",
    "        self.delta = error * (self.h * (1 - self.h))\n",
    "        \n",
    "        #Use the delta output  to figure out how much our hidden layer contributed to the output error \n",
    "        #by performing a dot product with our weight matrix\n",
    "        error = self.delta.dot(self.w.T)         \n",
    "        return error\n",
    "    \n",
    "    def step(self,h):      \n",
    "        self.w += h.T.dot(self.delta)        \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, outdim, activation = sigmoid):        \n",
    "        super().__init__(outdim,activation)        \n",
    "      \n",
    "        \n",
    "    def forward(self,x):        \n",
    "        #linear \n",
    "        h = np.dot(x, self.w) \n",
    "        return super().forward(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([    \n",
    "    Dense(10),    \n",
    "    Dense(1)    \n",
    "], epochs =3001, lr= 0.001)(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 0 Loss:0.8095493812635659 Error: 0.49609260484007356\n",
      "Epoch# 1000 Loss:0.04179670525777059 Error: 0.0409134589822716\n",
      "Epoch# 2000 Loss:0.023069436970873734 Error: 0.02279930553572503\n",
      "Epoch# 3000 Loss:0.0172455911627992 Error: 0.017094520819156633\n"
     ]
    }
   ],
   "source": [
    "model.fit()\n",
    "\n",
    "#we can see the loss geeting reduced on each iterations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let try to extent the sequential class to better predict the hand written digits in a next session"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
