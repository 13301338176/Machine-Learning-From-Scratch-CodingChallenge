{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout is a regularization techinque which can simply prevent overfitting. It drops out some nodes/neuron randomly during training. This helps in avoiding the network to closely align with the input samples(overfitting).  we can even call the dropout as ensemble methods or bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Ensemble Methods or Bagging? Why we call Dropout is one of them?\n",
    "\n",
    "Bagging or Ensemble is an idea to train several different models independent of each other and vote on all model outputs to choose the prediction. \n",
    "\n",
    "### How come Ensemble Methods generalize to the test set ?\n",
    "\n",
    "Before answering this question, let's define how to choose a different model.\n",
    "\n",
    "1) Using a Diffirent alogrithms or different hyper parameters\n",
    "2) Using a different constructed datasets(a subsets) from original datasets\n",
    "\n",
    "As per my exploration., the point 2 provides better generalization but there is no proper definition by it. The objective of point 2 here is to choose different subsets of samples constructed from orginal dataset of same size which means there is a high probability that each dataset missing some examples from original dataset and contains several duplicate samples.Remembers One classic example given by ian goodfellow in his deep learning book. say we need to predict the number 8, Model-1 with distribution 8,6,8 where it learns circle/loop on top is number 8. Model-2 with distribution 9 , 9 , 8 learns circle/loop on bottom is number 8. if we combine and mean the score of 2 models we get the prediction 8. Since each model has slightly different features from one another , this approach seems to be generalize well with test set.\n",
    "\n",
    "### What is the Problem with the ensemble methods stated above? \n",
    "    \n",
    "Simple, the more memory and computations is needed especially for larger network since it need to train multiple models for prediction. What if we create a approximation of this process in a single training loop i mean in O(N) loop. That's where drop out comes in.\n",
    "\n",
    "### What is Dropout  and how does it can be acheived?\n",
    "\n",
    "Dropout provides an inexpensive approximation to training and evaluating the bagged ensemble of exponentially many neural networks. Here our objective is to drop some percentage(is a hyperparameter to be configured) of neurons/node during forward propagation. So to acheive this we create mask vector usually a binomial vector with 0's and 1's and multiply it with the layer outcome. The zeros in mask vector helps to randomly drop features/neurons from the given layer. In other words, Dropout is a regularization technique where during each iteration of gradient descent, we drop a set of neurons selected at random. By drop, what we mean is that we essentially act as if they do not exist.\n",
    "\n",
    "Each neuron is dropped at random with some fixed probability 1-p, and kept with probability p. The value p may be different for each layer in the neural network. A value of 0.5 for the hidden layers, and 0.0 for input layer (no dropout) has been shown to work well on a wide range of tasks [1].\n",
    "\n",
    "During evaluation (and prediction), we do not ignore any neurons, i.e. no dropout is applied. Instead, the output of each neuron is multiplied by p. This is done so that the input to the next layer has the same expected value.\n",
    "\n",
    "To state with the real world example, from the book of deep learning. the power of droput arises from the fact that the masking noise is applied to hidden units. If the model learns a hidden unit h, that detects a face by finding the nose, then dropping h corresponds to erasing the information that there is a nose in the image. The model must learn another h, that either redundantly encodes the presence of a nose or detects the face by another feature, such as the mouth.\n",
    "\n",
    "Also, it said that dropout is less effective with extremely few labeled training samples. \n",
    "\n",
    "### Let's start implementing the dropout , for that let's copy our sequential class from last session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ipdb\n",
    "\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#This cell block has the List of Activation Functions with derivatives\n",
    "class sigmoid():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self,h):\n",
    "        return 1/(1+np.exp(-h)) \n",
    "    \n",
    "    def derivative(self,h):\n",
    "        return h * (1-h)   \n",
    "    \n",
    "    def diff(self, h ,y):\n",
    "        return y - h\n",
    "\n",
    "class relu():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self,h):\n",
    "        return h * (h >0)\n",
    "    \n",
    "    def derivative(self,h):\n",
    "        return 1. * (h >0)\n",
    "\n",
    "\n",
    "class tanh():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self,h):\n",
    "        return np.tanh(h)\n",
    "    \n",
    "    def derivative(self,h):\n",
    "        return 1. - np.power(h,2)\n",
    "\n",
    "class leakyrelu():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self,h, alpha=0.1):\n",
    "        return np.maximum(h ,alpha)\n",
    "    \n",
    "    def derivative(self,h):\n",
    "        d =  1. (h>0)\n",
    "        d[d <= 0] = alpha\n",
    "        return d       \n",
    "#softmax function as an activation function that maps the accumulated evidences to a probability distribution over the classes\n",
    "class softmax():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def __call__(self,h):\n",
    "        expo = np.exp(h)\n",
    "        result = expo/np.sum(expo,axis=1, keepdims=True)  \n",
    "        return result\n",
    "    \n",
    "    def derivative(self,h):\n",
    "        return None\n",
    "    \n",
    "    def diff(self, h, y):\n",
    "        dscore = h\n",
    "        dscore[range(len(y)), y.ravel()-1] -= 1        \n",
    "        dscore /= len(y)\n",
    "        return dscore\n",
    "\n",
    "#This Cell Block has List of Loss functions\n",
    "def binaryLoss(y, p,lweights = None, reg=1e-3):\n",
    "    return np.mean(-(y * np.log(p) + (1-y)*np.log(1-p)))    \n",
    "#y -actual output p - predicted output reg - regularization strength\n",
    "def crossEntropyForSoftMax(y, p,lweights = None, reg=1e-3): \n",
    "    #select the right propbolity for loss  \n",
    "    correct_prob = -np.log(p[range(len(y)), y.ravel()-1])\n",
    "    dataloss = np.sum(correct_prob)/len(y)   \n",
    "    \n",
    "    #regularization can be defined by 1/2 * Reg * np.sum(w*2)\n",
    "    regloss= 0\n",
    "    \n",
    "    if lweights is not None:\n",
    "        for weight in lweights:\n",
    "            regloss +=  0.5*reg* np.sum(np.square(weight))\n",
    "        \n",
    "    return dataloss+regloss  \n",
    "\n",
    "\n",
    "class Sequential:\n",
    "    def __init__(self, layers, epochs, lr,  loss = binaryLoss, reg =1e-3):\n",
    "        self.layers,self.epochs, self.lr,self.loss,self.reg = layers,epochs, lr ,loss,reg\n",
    "        \n",
    "    def __call__(self, X, y,X_valid, y_valid): \n",
    "        #assign weights\n",
    "        self.X,self.y,self.X_valid,self.y_valid = X,y,X_valid, y_valid    \n",
    "        \n",
    "        inputdim = X.shape[1]\n",
    "        np.random.seed(0)     \n",
    "        #initialize layers\n",
    "        for layer in self.layers:\n",
    "            inputdim = layer(inputdim,self.lr, self.reg) \n",
    "            \n",
    "        return self    \n",
    "    \n",
    "    def predict(self, X_input, y_input, training = False):\n",
    "        \n",
    "        h = X_input  \n",
    "        layerweigths = []\n",
    "        #compute hidden units\n",
    "        for layer in self.layers:  \n",
    "            h = layer.forward(h,training) \n",
    "            layerweigths.append(layer.w)\n",
    "        \n",
    "        loss = self.loss(y_input, h, layerweigths, self.reg)         \n",
    "        \n",
    "        return h,loss \n",
    "    \n",
    "    \n",
    "          \n",
    "    def fit(self):  \n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            \n",
    "            if((i%1000) == 0):\n",
    "                valid_h, valid_loss = self.predict(self.X_valid,self.y_valid)     \n",
    "                h, loss = self.predict(self.X,self.y, True)  \n",
    "                train_accuracy = np.mean(np.argmax(h ,axis=1)+1 == self.y.ravel())\n",
    "                val_accuracy = np.mean(np.argmax(valid_h ,axis=1)+1 == self.y_valid.ravel())\n",
    "               \n",
    "                print(f'Epoch# {i} Training Loss:{loss} Validation Loss: {valid_loss} Training Accuracy:{train_accuracy} Validation Accuracy:{val_accuracy}') \n",
    "            else :    h, loss = self.predict(self.X,self.y, True)  \n",
    "                \n",
    "            #compute the error  \n",
    "            error = self.layers[-1].activation.diff(h, self.y) \n",
    "            \n",
    "            #back propagate the error - this formula is influenced by andrew ng course  \n",
    "            for i in reversed(range(0,len(self.layers))):  \n",
    "                h = self.X if i == 0 else self.layers[i-1].h\n",
    "                error = self.layers[i].backward(error, h)          \n",
    "               \n",
    "            #update the weights   \n",
    "            for layer in reversed(self.layers):\n",
    "                layer.step()                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We going to provide option to drop out in our layer class, where the forward method will accept 2 extra parameters one is dropout probabilty, other is flag to state whether it is training or not. since dropout vector to be mulitplied only in training mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "    \n",
    "\n",
    "class Layer(ABC):\n",
    "    def __init__(self,   outdim, activation, drop_prob=None): \n",
    "        self.outdim = outdim\n",
    "        self.activation = activation\n",
    "        self.dropoutprob = drop_prob\n",
    "          \n",
    "        \n",
    "    def __call__(self, inputdim, lr= 0.01, reg=1e-3):  \n",
    "        self.inputdim,self.lr,self.reg= inputdim,lr,reg         \n",
    "        self.w = ( 0.01 * np.random.random((self.inputdim, self.outdim)))\n",
    "        self.b = (0.01 * np.random.random((1,self.outdim)))       \n",
    "        return self.outdim\n",
    "        \n",
    "   \n",
    "    def forward(self, x, training = False):    \n",
    "        h = self.activation(x)    \n",
    "        if(training is False):            \n",
    "            self.h =  h if self.dropoutprob is None else self.dropoutprob * h\n",
    "        elif(self.dropoutprob is not None): \n",
    "            self.mask = np.random.binomial(1, 1 - self.dropoutprob, size=x.shape)/self.dropoutprob\n",
    "            self.h = h *  self.mask\n",
    "            \n",
    "        return self.h\n",
    "    \n",
    "    def backward(self, d, h):   \n",
    "        #given an output value from a neuron, we need to calculate it’s slope.\n",
    "      \n",
    "        #Apply the derivative of our activation function to the output layer error\n",
    "        derivative = self.activation.derivative(self.h)\n",
    "        \n",
    "        if(derivative is None):  delta = d \n",
    "        else: delta = d * derivative if self.dropoutprob is None  else d * derivative * self.dropoutprob           \n",
    "        \n",
    "            \n",
    "        self.dw = np.dot(h.T, delta)\n",
    "        self.db = np.sum(delta, axis =0, keepdims=True)      \n",
    "        \n",
    "        #Use the delta output  to figure out how much our hidden layer contributed to the output error \n",
    "        #by performing a dot product with our weight matrix\n",
    "        error = delta.dot(self.w.T)               \n",
    "        \n",
    "        self.dw += self.reg * self.w    \n",
    "        \n",
    "        return error\n",
    "    \n",
    "    def step(self):      \n",
    "        self.w +=  -self.lr * self.dw\n",
    "        self.b +=  -self.lr * self.db\n",
    "        \n",
    "class Dense(Layer):\n",
    "    def __init__(self, outdim, activation = sigmoid,drop_prob=None):        \n",
    "        super().__init__(outdim,activation,drop_prob)        \n",
    "      \n",
    "        \n",
    "    def forward(self,x,training = False):        \n",
    "        #linear \n",
    "        h = np.dot(x, self.w)  + self.b\n",
    "        return super().forward(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 400)\n",
      "(5000, 1)\n"
     ]
    }
   ],
   "source": [
    "data = loadmat(\"data\\ex3data1.mat\")\n",
    "print(data['X'].shape)\n",
    "print(data['y'].shape)\n",
    "\n",
    "X = data['X']\n",
    "y =  data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 400)\n",
      "(1000, 400)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            X,y, test_size=0.20, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([    \n",
    "    Dense(100, activation = relu(), drop_prob = 0.25),   \n",
    "    Dense(10, activation = softmax())    \n",
    "], epochs =10000, lr= 0.1, reg= 1e-3, loss= crossEntropyForSoftMax)(X_train,y_train,X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 0 Training Loss:2.3033357673476655 Validation Loss: 2.3035524398788154 Training Accuracy:0.10375 Validation Accuracy:0.085\n",
      "Epoch# 1000 Training Loss:0.8832151542393043 Validation Loss: 0.8959211433730334 Training Accuracy:0.801 Validation Accuracy:0.784\n",
      "Epoch# 2000 Training Loss:0.5078076851084635 Validation Loss: 0.5610542637853289 Training Accuracy:0.889 Validation Accuracy:0.879\n",
      "Epoch# 3000 Training Loss:0.4417533138054067 Validation Loss: 0.5151852124058112 Training Accuracy:0.912 Validation Accuracy:0.895\n",
      "Epoch# 4000 Training Loss:0.41323353836784255 Validation Loss: 0.5001637084673154 Training Accuracy:0.92775 Validation Accuracy:0.899\n",
      "Epoch# 5000 Training Loss:0.3971062961958248 Validation Loss: 0.4936247708886639 Training Accuracy:0.931 Validation Accuracy:0.906\n",
      "Epoch# 6000 Training Loss:0.3868218364961822 Validation Loss: 0.4903509228238794 Training Accuracy:0.93725 Validation Accuracy:0.91\n",
      "Epoch# 7000 Training Loss:0.3796409285714771 Validation Loss: 0.4886172989430282 Training Accuracy:0.93875 Validation Accuracy:0.912\n",
      "Epoch# 8000 Training Loss:0.3743087554494142 Validation Loss: 0.4874918252885642 Training Accuracy:0.9425 Validation Accuracy:0.912\n",
      "Epoch# 9000 Training Loss:0.37011379735054584 Validation Loss: 0.4867571324139325 Training Accuracy:0.945 Validation Accuracy:0.914\n"
     ]
    }
   ],
   "source": [
    "model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Droupout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch# 0 Training Loss:2.3036310355372773 Validation Loss: 2.3041550030303695 Training Accuracy:0.10475 Validation Accuracy:0.085\n",
      "Epoch# 1000 Training Loss:0.2948425367948705 Validation Loss: 0.3996565177455286 Training Accuracy:0.93075 Validation Accuracy:0.907\n",
      "Epoch# 2000 Training Loss:0.2308697192764801 Validation Loss: 0.3840779523124036 Training Accuracy:0.95175 Validation Accuracy:0.92\n",
      "Epoch# 3000 Training Loss:0.19830037834508418 Validation Loss: 0.38122552253673886 Training Accuracy:0.9685 Validation Accuracy:0.919\n",
      "Epoch# 4000 Training Loss:0.17708025333789545 Validation Loss: 0.3790356183890529 Training Accuracy:0.977 Validation Accuracy:0.923\n",
      "Epoch# 5000 Training Loss:0.16288755469974536 Validation Loss: 0.37694162154958005 Training Accuracy:0.983 Validation Accuracy:0.922\n",
      "Epoch# 6000 Training Loss:0.15380883906196735 Validation Loss: 0.37555439485528996 Training Accuracy:0.9895 Validation Accuracy:0.926\n",
      "Epoch# 7000 Training Loss:0.14805186356499744 Validation Loss: 0.37380542800325733 Training Accuracy:0.994 Validation Accuracy:0.926\n",
      "Epoch# 8000 Training Loss:0.14438294590510953 Validation Loss: 0.372050384544171 Training Accuracy:0.99575 Validation Accuracy:0.93\n",
      "Epoch# 9000 Training Loss:0.14188278372593305 Validation Loss: 0.3697176141653569 Training Accuracy:0.997 Validation Accuracy:0.933\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([    \n",
    "    Dense(100, activation = relu()),   \n",
    "    Dense(10, activation = softmax())    \n",
    "], epochs =10000, lr= 0.1, reg= 1e-3, loss= crossEntropyForSoftMax)(X_train,y_train,X_valid, y_valid)\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Still the loss is decreasing and it means there will be a increase in accuracy when epochs are increased, if you see here the dropout not performs better when compare to the accuracy without dropout. It will be more effective with slightly deeper networks and with more training samples. But you can notice the difference between the training accuracy and validation accuracy is minimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the next session, let see how do we perform batch norm(a normalization technique) on hidden layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
